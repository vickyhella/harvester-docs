"use strict";(self.webpackChunkharvester_docs=self.webpackChunkharvester_docs||[]).push([[7964],{9323:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"nic-naming-scheme","metadata":{"permalink":"/harvester-docs/zh/kb/nic-naming-scheme","editUrl":"https://github.com/vickyhella/harvester-docs/edit/main/kb/kb/2022-04-06/nic_naming_scheme.md","source":"@site/kb/2022-04-06/nic_naming_scheme.md","title":"NIC Naming Scheme","description":"NIC Naming Scheme changed after upgrading to v1.0.1","date":"2022-04-06T00:00:00.000Z","formattedDate":"2022\u5e744\u67086\u65e5","tags":[{"label":"network","permalink":"/harvester-docs/zh/kb/tags/network"}],"readingTime":1.825,"hasTruncateMarker":false,"authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"frontMatter":{"title":"NIC Naming Scheme","descripion":"NIC Naming Scheme Change","slug":"nic-naming-scheme","authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"tags":["network"],"hide_table_of_contents":false},"nextItem":{"title":"Multiple NICs VM Connectivity","permalink":"/harvester-docs/zh/kb/multiple-nics-vm-connectivity"}},"content":"## NIC Naming Scheme changed after upgrading to v1.0.1\\n\\n`systemd` in OpenSUSE Leap 15.3 which is the base OS of Harvester is upgraded to `246.16-150300.7.39.1`. In this version, `systemd` will enable additional naming scheme `sle15-sp3` which is `v238` with `bridge_no_slot`. When there is a PCI bridge associated with NIC, `systemd` will never generate `ID_NET_NAME_SLOT` and naming policy in `/usr/lib/systemd/network/99-default.link` will fallback to `ID_NET_NAME_PATH`. According to this change, NIC names might be changed in your Harvester nodes during the upgrade process from `v1.0.0` to `v1.0.1-rc1` or above, and it will cause network issues that are associated with NIC names.\\n\\n## Effect Settings and Workaround\\n\\n### Startup Network Configuration\\n\\nNIC name changes will need to update the name in `/oem/99_custom.yaml`. You could use [migration script](https://github.com/harvester/upgrade-helpers/blob/main/hack/udev_v238_sle15-sp3.py) to change the NIC names which are associated with a PCI bridge.\\n\\n:::tip\\nYou could find an identical machine to test naming changes before applying the configuration to production machines\\n:::\\n\\nYou could simply execute the script with root account in `v1.0.0` via\\n```bash\\n# python3 udev_v238_sle15-sp3.py\\n```\\n\\nIt will output the patched configuration to the screen and you could compare it to the original one to ensure there is no exception. (e.g. We could use `vimdiff` to check the configuration)\\n```bash\\n# python3 udev_v238_sle15-spe3.py > /oem/test\\n# vimdiff /oem/test /oem/99_custom.yaml\\n```\\n\\nAfter checking the result, we could execute the script with `--really-want-to-do` to override the configuration. It will also back up the original configuration file with a timestamp before patching it.\\n```bash\\n# python3 udev_v238_sle15-sp3.py --really-want-to-do\\n```\\n\\n### Harvester VLAN Network Configuration\\n\\nIf your VLAN network is associated with NIC name directly without `bonding`, you will need to migrate `ClusterNetwork` and `NodeNetwork` with the previous section together.\\n\\n:::note\\nIf your VLAN network is associated with the `bonding` name in `/oem/99_custom.yaml`, you could skip this section.\\n:::\\n\\n#### Modify ClusterNetworks\\n\\nYou need to modify `ClusterNetworks` via \\n```bash\\n$ kubectl edit clusternetworks vlan\\n```\\nsearch this pattern\\n```yaml\\nconfig:\\n  defaultPhysicalNIC: <Your NIC name>\\n```\\nand change to new NIC name\\n\\n#### Modify NodeNetworks\\n\\nYou need to modify `NodeNetworks` via\\n```bash\\n$ kubectl edit nodenetworks <Node name>-vlan\\n```\\nsearch this pattern\\n```yaml\\nspec:\\n  nic: <Your NIC name>\\n```\\nand change to new NIC name"},{"id":"multiple-nics-vm-connectivity","metadata":{"permalink":"/harvester-docs/zh/kb/multiple-nics-vm-connectivity","editUrl":"https://github.com/vickyhella/harvester-docs/edit/main/kb/kb/2022-03-10/multiple_nics_vm_connectivity.md","source":"@site/kb/2022-03-10/multiple_nics_vm_connectivity.md","title":"Multiple NICs VM Connectivity","description":"What is the default behavior of a VM with multiple NICs","date":"2022-03-10T00:00:00.000Z","formattedDate":"2022\u5e743\u670810\u65e5","tags":[{"label":"vm","permalink":"/harvester-docs/zh/kb/tags/vm"},{"label":"network","permalink":"/harvester-docs/zh/kb/tags/network"}],"readingTime":3.955,"hasTruncateMarker":false,"authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"frontMatter":{"title":"Multiple NICs VM Connectivity","descripion":"How to deal VMs with multiple NICs in Harvester","slug":"multiple-nics-vm-connectivity","authors":[{"name":"Date Huang","title":"Software Engineer","url":"https://github.com/tjjh89017","image_url":"https://github.com/tjjh89017.png","imageURL":"https://github.com/tjjh89017.png"}],"tags":["vm","network"],"hide_table_of_contents":false},"prevItem":{"title":"NIC Naming Scheme","permalink":"/harvester-docs/zh/kb/nic-naming-scheme"},"nextItem":{"title":"VM Scheduling","permalink":"/harvester-docs/zh/kb/vm-scheduling"}},"content":"## What is the default behavior of a VM with multiple NICs\\n\\nIn [some scenarios](https://github.com/harvester/harvester/issues/1059), you\'ll setup two or more NICs in your VM to serve different networking purposes. If all networks are setup by default with DHCP, you might get random connectivity issues. And while it might get fixed after rebooting the VM, it still will lose connection randomly after some period.\\n\\n## How-to identify connectivity issues\\n\\nIn a Linux VM, you can use commands from the `iproute2` package to identify the default route.\\n\\nIn your VM, execute the following command:\\n```bash\\nip route show default\\n```\\n:::tip\\nIf you get the `access denied` error, please run the command using `sudo`\\n:::\\n    \\nThe output of this command will only show the default route with the gateway and VM IP of the primary network interface (`eth0` in the example below).\\n```\\ndefault via <Gateway IP> dev eth0 proto dhcp src <VM IP> metric 100\\n```\\n\\nHere is the full example:\\n```\\n$ ip route show default\\ndefault via 192.168.0.254 dev eth0 proto dhcp src 192.168.0.100 metric 100\\n```\\n\\nHowever, if the issue covered in this KB occurs, you\'ll only be able to connect to the VM via the VNC or serial console.\\n\\nOnce connected, you can run again the same command as before:\\n```bash\\n$ ip route show default\\n```\\n\\nHowever, this time you\'ll get a default route with an incorrect gateway IP.\\nFor example:\\n```\\ndefault via <Incorrect Gateway IP> dev eth0 proto dhcp src <VM\'s IP> metric 100\\n```\\n\\n## Why do connectivity issues occur randomly\\n\\nIn a standard setup, cloud-based VMs typically use DHCP for their NICs configuration. It will set an IP and a gateway for each NIC. Lastly, a default route to the gateway IP will also be added, so you can use its IP to connect to the VM.\\n\\nHowever, Linux distributions start multiple DHCP clients at the same time and do not have a **priority** system. This means that if you have two or more NICs configured with DHCP, the client will enter a **race condition** to configure the default route. And depending on the currently running Linux distribution DHCP script, there is no guarantee which default route will be configured.\\n\\nAs the default route might change in every DHCP renewing process or after every OS reboot, this will create network connectivity issues.\\n\\n## How to avoid the random connectivity issues\\n\\nYou can easily avoid these connectivity issues by having only one NIC attached to the VM and having only one IP and one gateway configured.\\n\\nHowever, for VMs in more complex infrastructures, it is often not possible to use just one NIC. For example, if your infrastructure has a storage network and a service network. For security reasons, the storage network will be isolated from the service network and have a separate subnet. In this case, you must have two NICs to connect to both the service and storage networks.\\n\\nYou can choose a solution below that meets your requirements and security policy.\\n\\n### Disable DHCP on secondary NIC\\n\\nAs mentioned above, the problem is caused by a `race condition` between two DHCP clients. One solution to avoid this problem is to disable DHCP for all NICs and configure them with static IPs only. Likewise, you can configure the secondary NIC with a static IP and keep the primary NIC enabled with DHCP.\\n\\n1. To configure the primary NIC with a static IP (`eth0` in this example), you can edit the file `/etc/sysconfig/network/ifcfg-eth0` with the following values:\\n\\n```\\nBOOTPROTO=\'static\'\\nIPADDR=\'192.168.0.100\'\\nNETMASK=\'255.255.255.0\'\\n```\\n\\nAlternatively, if you want to reserve the primary NIC using DHCP (`eth0` in this example), use the following values instead:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'yes\'\\n```\\n\\n\\n2. You need to configure the default route by editing the file `/etc/sysconfig/network/ifroute-eth0` (if you configured the primary NIC using DHCP, skip this step):\\n\\n\\n```\\n# Destination  Dummy/Gateway  Netmask  Interface\\ndefault        192.168.0.254  -        eth0\\n```\\n\\n:::caution\\nDo not put other default route for your secondary NIC\\n:::\\n    \\n3. Finally, configure a static IP for the secondary NIC by editing the file `/etc/sysconfig/network/ifcfg-eth1`:\\n\\n```\\nBOOTPROTO=\'static\'\\nIPADDR=\'10.0.0.100\'\\nNETMASK=\'255.255.255.0\'\\n```\\n\\n#### Cloud-Init config\\n\\n```yaml\\nnetwork:\\n  version: 1\\n  config:\\n    - type: physical\\n      name: eth0\\n      subnets:\\n        - type: dhcp\\n    - type: physical\\n      name: eth1\\n      subnets:\\n        - type: static\\n          address: 10.0.0.100/24\\n```\\n   \\n### Disable secondary NIC default route from DHCP\\n\\nIf your secondary NIC requires to get its IP from DHCP, you\'ll need to disable the secondary NIC default route configuration.\\n\\n1. Confirm that the primary NIC configures its default route in the file `/etc/sysconfig/network/ifcfg-eth0`:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'yes\'\\n```\\n\\n2. Disable the secondary NIC default route configuration by editing the file `/etc/sysconfig/network/ifcfg-eth1`:\\n\\n```\\nBOOTPROTO=\'dhcp\'\\nDHCLIENT_SET_DEFAULT_ROUTE=\'no\'\\n```\\n\\n#### Cloud-Init config\\n\\nThis solution is not available in Cloud-Init. Cloud-Init didn\'t allow any option for DHCP."},{"id":"vm-scheduling","metadata":{"permalink":"/harvester-docs/zh/kb/vm-scheduling","editUrl":"https://github.com/vickyhella/harvester-docs/edit/main/kb/kb/2022-03-07/vm-scheduling.md","source":"@site/kb/2022-03-07/vm-scheduling.md","title":"VM Scheduling","description":"How does Harvester schedule VMs?","date":"2022-03-07T00:00:00.000Z","formattedDate":"2022\u5e743\u67087\u65e5","tags":[{"label":"vm","permalink":"/harvester-docs/zh/kb/tags/vm"},{"label":"scheduling","permalink":"/harvester-docs/zh/kb/tags/scheduling"}],"readingTime":15.44,"hasTruncateMarker":false,"authors":[{"name":"PoAn Yang","title":"Software Engineer","url":"https://github.com/FrankYang0529","image_url":"https://github.com/FrankYang0529.png","imageURL":"https://github.com/FrankYang0529.png"}],"frontMatter":{"title":"VM Scheduling","description":"How does Harvester schedule VMs?","slug":"vm-scheduling","authors":[{"name":"PoAn Yang","title":"Software Engineer","url":"https://github.com/FrankYang0529","image_url":"https://github.com/FrankYang0529.png","imageURL":"https://github.com/FrankYang0529.png"}],"tags":["vm","scheduling"],"hide_table_of_contents":false},"prevItem":{"title":"Multiple NICs VM Connectivity","permalink":"/harvester-docs/zh/kb/multiple-nics-vm-connectivity"}},"content":"## How does Harvester schedule a VM?\\n\\nHarvester doesn\'t directly schedule a VM in Kubernetes, it relies on [KubeVirt](http://kubevirt.io/) to create the custom resource `VirtualMachine`. When the request to create a new VM is sent, a `VirtualMachineInstance` object is created and it creates the corresponding `Pod`.\\n\\nThe whole VM creation processt leverages `kube-scheduler`, which allows Harvester to use `nodeSelector`, `affinity`, and resources request/limitation to influence where a VM will be deployed.\\n\\n## How does kube-scheduler decide where to deploy a VM?\\n\\nFirst, `kube-scheduler` finds Nodes available to run a pod. After that, `kube-scheduler` scores each available Node by a list of [plugins](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins) like [ImageLocality](https://github.com/kubernetes/kubernetes/blob/v1.22.7/pkg/scheduler/framework/plugins/imagelocality/image_locality.go), [InterPodAffinity](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins/interpodaffinity), [NodeAffinity](https://github.com/kubernetes/kubernetes/tree/v1.22.7/pkg/scheduler/framework/plugins/nodeaffinity), etc. \\n\\nFinally, `kube-scheduler` calculates the scores from the plugins results for each Node, and select the Node with the highest score to deploy the Pod.\\n\\nFor example, let\'s say  we have a three nodes Harvester cluster with 6 cores CPU and 16G RAM each, and we want to deploy a VM with 1 CPU and 1G RAM (without resources overcommit). \\n\\n`kube-scheduler` will summarize the scores, as displayed in  _Table 1_ below, and will select the node with the highest score, `harvester-node-2` in this case, to deploy the VM.\\n\\n<details>\\n  <summary>kube-scheduler logs</summary>\\n\\n```\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 46,\\n\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm-without-overcommit-75q9b -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 37,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=37\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-2\\" score=1000437\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm-without-overcommit-75q9b\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm-without-overcommit-75q9b\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm-without-overcommit-75q9b\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n**Table 1 - kube-scheduler scores example**\\n\\n|                                 | harvester-node-0 | harvester-node-1 | harvester-node-2 |\\n|:-------------------------------:|:----------------:|:----------------:|:----------------:|\\n| ImageLocality                   |               54 |               54 |               54 |\\n| InterPodAffinity                |                0 |                0 |                0 |\\n| NodeResourcesLeastAllocated     |                4 |               34 |               37 |\\n| NodeAffinity                    |                0 |                0 |                0 |\\n| NodePreferAvoidPods             |          1000000 |          1000000 |          1000000 |\\n| PodTopologySpread               |              200 |              200 |              200 |\\n| TaintToleration                 |              100 |              100 |              100 |\\n| NodeResourcesBalancedAllocation |                0 |               45 |               46 |\\n| Total                           |          1000358 |          1000433 |          1000437 |\\n\\n## Why VMs are distributed unevenly with overcommit?\\n\\nWith resources overcommit, Harvester modifies the resources request. By default, the `overcommit` configuration is `{\\"cpu\\": 1600, \\"memory\\": 150, \\"storage\\":  200}`. This means that if we request a VM with 1 CPU and 1G RAM, its `resources.requests.cpu` will become `62m`. \\n\\n:::note\\nThe unit suffix `m` stands for \\"thousandth of a core.\\"\\n:::\\n\\nTo explain it, let\'s take the case of CPU overcommit. The default value of 1 CPU is equal to 1000m CPU, and with the default overcommit configuration of `\\"cpu\\": 1600`, the CPU resource will be 16x smaller. Here is the calculation: `1000m * 100 / 1600 = 62m`.\\n\\nNow, we can see how overcommitting influences `kube-scheduler` scores.\\n\\nIn this example, we use a three nodes Harvester cluster with 6 cores and 16G RAM each. We will deploy two VMs with 1 CPU and 1G RAM, and we will compare the scores for both cases of \\"with-overcommit\\" and \\"without-overcommit\\" resources. \\n\\nThe results of both tables _Table 2_ and _Table 3_ can be explained as follow:\\n\\nIn the \\"with-overcommit\\" case, both VMs are deployed on `harvester-node-2`, however in the \\"without-overcommit\\" case, the VM1 is deployed on `harvester-node-2`, and VM2 is deployed on `harvester-node-1`. \\n\\nIf we look at the detailed scores, we\'ll see a variation of `Total Score` for `harvester-node-2` from `1000459` to `1000461` in the \\"with-overcommit\\" case, and `1000437` to `1000382` in the \\"without-overcommit case\\". It\'s because resources overcommit influences `request-cpu` and `request-memory`. \\n\\nIn the \\"with-overcommit\\" case, the `request-cpu` changes from `4412m` to `4474m`. The difference between the two numbers is `62m`, which is what we calculated above. However, in the \\"without-overcommit\\" case, we send **real** requests to `kube-scheduler`, so the `request-cpu` changes from `5350m` to `6350m`.\\n\\nFinally, since most plugins give the same scores for each node except `NodeResourcesBalancedAllocation` and `NodeResourcesLeastAllocated`, we\'ll see a difference of these two scores for each node.\\n\\nFrom the results, we can see the overcommit feature influences the final score of each Node, so VMs are distributed unevenly. Although the `harvester-node-2` score for VM 2 is higher than VM 1, it\'s not always increasing. In _Table 4_, we keep deploying VM with 1 CPU and 1G RAM, and we can see the score of `harvester-node-2` starts decreasing from 11th VM. The behavior of `kube-scheduler` depends on your cluster resources and the workload you deployed.\\n\\n<details>\\n  <summary>kube-scheduler logs for vm1-with-overcommit</summary>\\n\\n```\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 0,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 58,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4412 memory:5581918208] ,score 59,\\n\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 5,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 43,\\nvirt-launcher-vm1-with-overcommit-ljlmq -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4412 memory:5581918208] ,score 46,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=5\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=43\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=58\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=59\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-0\\" score=1000359\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-1\\" score=1000455\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-2\\" score=1000459\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-ljlmq\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-ljlmq\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm1-with-overcommit-ljlmq\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm2-with-overcommit</summary>\\n\\n```\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 0,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 58,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4474 memory:6476701696] ,score 64,\\n\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9022 memory:14807289856] ,score 5,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4622 memory:5992960000] ,score 43,\\nvirt-launcher-vm2-with-overcommit-pwrx4 -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:4474 memory:6476701696] ,score 43,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=58\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=64\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=5\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=43\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=43\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-0\\" score=1000359\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-1\\" score=1000455\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-2\\" score=1000461\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-with-overcommit-pwrx4\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-with-overcommit-pwrx4\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm2-with-overcommit-pwrx4\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm1-without-overcommit</summary>\\n\\n```\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 46,\\n\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm1-with-overcommit-6xqmq -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5350 memory:5941231616] ,score 37,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=37\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=46\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-2\\" score=1000437\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-6xqmq\\", node \\"harvester-node-2\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm1-with-overcommit-6xqmq\\", node \\"harvester-node-2\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm1-with-overcommit-6xqmq\\" node=\\"harvester-node-2\\"\\n```\\n</details>\\n\\n<details>\\n  <summary>kube-scheduler logs for vm2-without-overcommit</summary>\\n\\n```\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-0: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 0,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-1: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 45,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-2: NodeResourcesBalancedAllocation, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:6350 memory:7195328512] ,score 0,\\n\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-0: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:9960 memory:15166603264] ,score 4,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-1: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:5560 memory:6352273408] ,score 34,\\nvirt-launcher-vm2-without-overcommit-mf5vk -> harvester-node-2: NodeResourcesLeastAllocated, map of allocatable resources map[cpu:6000 memory:16776437760], map of requested resources map[cpu:6350 memory:7195328512] ,score 28,\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-0\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-1\\" score=200\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"PodTopologySpread\\" node=\\"harvester-node-2\\" score=200\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-0\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-1\\" score=100\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"TaintToleration\\" node=\\"harvester-node-2\\" score=100\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-1\\" score=45\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesBalancedAllocation\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-0\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-1\\" score=54\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"ImageLocality\\" node=\\"harvester-node-2\\" score=54\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"InterPodAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-0\\" score=4\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-1\\" score=34\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeResourcesLeastAllocated\\" node=\\"harvester-node-2\\" score=28\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-0\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-1\\" score=0\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodeAffinity\\" node=\\"harvester-node-2\\" score=0\\n\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-0\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-1\\" score=1000000\\n\\"Plugin scored node for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" plugin=\\"NodePreferAvoidPods\\" node=\\"harvester-node-2\\" score=1000000\\n\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-0\\" score=1000358\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-1\\" score=1000433\\n\\"Calculated node\'s final score for pod\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-2\\" score=1000382\\n\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-without-overcommit-mf5vk\\", node \\"harvester-node-1\\"\\nAssumePodVolumes for pod \\"default/virt-launcher-vm2-without-overcommit-mf5vk\\", node \\"harvester-node-1\\": all PVCs bound and nothing to do\\n\\"Attempting to bind pod to node\\" pod=\\"default/virt-launcher-vm2-without-overcommit-mf5vk\\" node=\\"harvester-node-1\\"\\n```\\n</details>\\n\\n**Table 2 - With Overcommit**\\n\\n|              VM 1 / VM 2              |          harvester-node-0 |        harvester-node-1 |        harvester-node-2 |\\n|:-------------------------------------:|--------------------------:|------------------------:|------------------------:|\\n|            request-cpu (m)            |               9022 / 9022 |             4622 / 4622 |             **4412** / **4474** |\\n|             request-memory            | 14807289856 / 14807289856 | 5992960000 / 5992960000 | **5581918208** / **6476701696** |\\n| NodeResourcesBalancedAllocation Score |                     0 / 0 |                 58 / 58 |                 **59** / **64** |\\n|   NodeResourcesLeastAllocated Score   |                     5 / 5 |                 43 / 43 |                 **46** / **43** |\\n| Other Scores                          |         1000354 / 1000354 |       1000354 / 1000354 |       1000354 / 1000354 |\\n|              Total Score              |         1000359 / 1000359 |       1000455 / 1000455 |       **1000459** / **1000461** |\\n\\n**Table 3 - Without Overcommit**\\n\\n|              VM 1 / VM 2              |          harvester-node-0 |        harvester-node-1 |        harvester-node-2 |\\n|:-------------------------------------:|--------------------------:|------------------------:|------------------------:|\\n|            request-cpu (m)            |               9960 / 9960 |             5560 / **5560** |             **5350** / 6350 |\\n|             request-memory            | 15166603264 / 15166603264 | 6352273408 / **6352273408** | **5941231616** / 7195328512 |\\n| NodeResourcesBalancedAllocation Score |                     0 / 0 |                 45 / **45** |                  **46** / 0 |\\n|   NodeResourcesLeastAllocated Score   |                     4 / 4 |                 34 / **34** |                 **37** / 28 |\\n| Other Scores                          |         1000354 / 1000354 |       1000354 / **1000354** |       **1000354** / 1000354 |\\n|              Total Score              |         1000358 / 1000358 |       1000358 / **1000433** |       **1000437** / 1000382 |\\n\\n**Table 4**\\n\\n| Score | harvester-node-0 | harvester-node-1 | harvester-node-2 |\\n|:-----:|-----------------:|-----------------:|-----------------:|\\n|  VM 1 |          1000359 |          1000455 |          1000459 |\\n|  VM 2 |          1000359 |          1000455 |          1000461 |\\n|  VM 3 |          1000359 |          1000455 |          1000462 |\\n|  VM 4 |          1000359 |          1000455 |          1000462 |\\n| VM 5  |          1000359 |          1000455 |          1000463 |\\n|  VM 6 |          1000359 |          1000455 |          1000465 |\\n| VM 7  |          1000359 |          1000455 |          1000466 |\\n| VM 8  |          1000359 |          1000455 |          1000467 |\\n| VM 9  |          1000359 |          1000455 |          1000469 |\\n| VM 10 |          1000359 |          1000455 |          1000469 |\\n| VM 11 |          1000359 |          1000455 |      **1000465** |\\n| VM 12 |          1000359 |          1000455 |      **1000457** |\\n\\n\\n## How to avoid uneven distribution of VMs?\\n\\nThere are many plugins in `kube-scheduler` which we can use to influence the scores. For example, we can add the `podAntiAffinity` plugin to avoid VMs with the same labels being deployed on the same node.\\n\\n```\\n  affinity:\\n    podAntiAffinity:\\n      preferredDuringSchedulingIgnoredDuringExecution:\\n      - podAffinityTerm:\\n          labelSelector:\\n            matchExpressions:\\n            - key: harvesterhci.io/creator\\n              operator: Exists\\n          topologyKey: kubernetes.io/hostname\\n        weight: 100\\n```\\n\\n## How to see scores in kube-scheduler?\\n\\n`kube-scheduler` is deployed as a static pod in Harvester. The file is under `/var/lib/rancher/rke2/agent/pod-manifests/kube-scheduler.yaml` in each Management Node. We can add `- --v=10` to the `kube-scheduler` container to show score logs.\\n\\n```\\nkind: Pod\\nmetadata:\\n  labels:\\n    component: kube-scheduler\\n    tier: control-plane\\n  name: kube-scheduler\\n  namespace: kube-system\\nspec:\\n  containers:\\n  - command:\\n    - kube-scheduler\\n    # ...\\n    - --v=10\\n```"}]}')}}]);